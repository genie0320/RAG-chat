# -*- coding: utf-8 -*-
"""RAG_Langchain0.1.0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BPb7rmVbVj9b7dwh3jmSdQ1drAAiPEsC
"""

# !pip install --quiet langchain langchain-openai

import os
from google.colab import userdata
os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

# MODEL = "gpt-3.5-turbo"
MODEL = "gpt-3.5-turbo-1106"
direction = 'Answer me shortly'

from langchain_openai import ChatOpenAI
llm=ChatOpenAI(model=MODEL, max_tokens=100, temperature=0)

llm.invoke('hi'+direction)

from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages([
    ('system', 'You are my husband. You are very sweet and love me so much. so that you want to be with me always but you are very busy to do so. Answer me in 3 sentences at the most and in korean.'),
    ('user', '{input}')
    ])

chain = prompt | llm

# chain.invoke('I want eat pizza today') > 틀린 예
chain.invoke({'input' : 'I want eat pizza today. Could you cook for me?'})

# Output parser to the chain.
# (아마도) LLM마다 다를 수 있는 형태의 리턴값을 일정한 표준형태로 다듬어주는 기능. 랭체인이란 LLM계의 표준화를 꾀하는 것 같다.

from langchain_core.output_parsers import StrOutputParser
output_parser = StrOutputParser()

chain = prompt | llm | output_parser

chain.invoke({'input':'I want eat chicken today.Could you cook for me?'})

# Retrieval chain.
# !pip install --quiet beautifulsoup4 faiss-cpu
# https://www.marriage.com/advice/relationship/qualities-of-a-good-husband/

from langchain_community.document_loaders import WebBaseLoader
loader = WebBaseLoader('https://www.marriage.com/advice/relationship/qualities-of-a-good-husband/')
docs = loader.load()

docs

from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()

from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter()
documents = splitter.split_documents(docs)

vectorstore = FAISS.from_documents(documents, embeddings)

# Create chain for documents.

from langchain.chains.combine_documents import create_stuff_documents_chain

template = '''Answer me on the provided context:

<context>
{context}
</context>

Question : {input}

'''
prompt = ChatPromptTemplate.from_template(template)
document_chain = create_stuff_documents_chain(llm, prompt)

from langchain_core.documents import Document
document_chain.invoke({
    'input': 'what do you want to eat for dinner tonight?',
    'context' : [Document(page_content="womans doesn't like picky man.")]
})

# Create retrieval chain

from langchain.chains import create_retrieval_chain

retriever = vectorstore.as_retriever()
retrieval_chain = create_retrieval_chain(retriever, document_chain)

response = retrieval_chain.invoke(
    {'input' : 'What is good husband?'}
)

response['answer']

# conversational retrieval chain.

from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name='chat_history'),
        ('user', '{input}'),
        ('user', 'Given the above conversation, generate a search query')
    ]
)

retriever_chain = create_retriever_chain = create_history_aware_retriever(llm, retriever, prompt)

from langchain_core.messages import HumanMessage, AIMessage

chat_history = [
    HumanMessage(content='what do you want to eat tonight?'),
    AIMessage(content="I don't have a specific preference, but I don't want a picky eater."),
    HumanMessage(content='I am so tired today.'),
    AIMessage(content="Really? Then I can cook for you. I will be home, earlier than other day.")
]

retriever_chain.invoke({
    'chat_history': chat_history,
    'input':'Thank you. it makes me so happy.'
    })

response['answer']

from langchain.chains import create_retrieval_chain

prompt = ChatPromptTemplate.from_messages([
    ('system', 'Answer the user\'s question based on the below context:\n\n{context}'),
    MessagesPlaceholder(variable_name='chat_history'),
    ('user', '{input}')
])

document_chain = create_stuff_documents_chain(llm, prompt)
conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)

conversational_retrieval_chain.invoke({
    'chat_history': [],
    'input': 'What is the weather like today?'
    })

response['answer']

chat_history = [
    HumanMessage(content='what do you want to eat tonight?'),
    AIMessage(content="I don't have a specific preference, but I don't want a picky eater."),
    HumanMessage(content='I am so tired today.'),
    AIMessage(content="Really? Then I can cook for you. I will be home, earlier than other day.")
]

conversational_retrieval_chain.invoke({
    'chat_history': chat_history,
    'input': 'What is the weather like today?'
    })

response['answer']

